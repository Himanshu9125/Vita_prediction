# -*- coding: utf-8 -*-
"""Eduaction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1depk47v34-d-Rlo40h_oRT4O-TcX5Vwb

# **Education Status**
"""

import pandas as pd
import seaborn as sns
import matplotlib
import numpy as np

from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression

from yellowbrick.features import RFECV
import statsmodels.formula.api as smf

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_absolute_error, accuracy_score

ds = pd.read_csv("Education_Status.csv")

def val(x):
    if(x == 'No Education'):
        return 0
    elif(x == 'Primary'):
        return 1
    elif(x == 'Middle'):
        return 2
    elif(x == 'Matriculate/Secondary'):
        return 3
    elif(x == 'Hr. Secondary/Intermediate/Pre-Universit'):
        return 4
    elif(x == 'Diploma'):
        return 5
    elif(x == 'Graduate'):
        return 6
    elif(x == 'Post Graduate and Above'):
        return 7
    else: return 8

def gender_encoding(i):
    if i in ['Female', 'Fema']:  # handle possible typos
        return 1
    else:
        return 0

cataegory = pd.Series([])
for ind,row in ds.iterrows():
    ds.loc[ind,"Cataegory"] = val(ds.loc[ind,"Type"])
ds = ds.astype({"Cataegory": int})

ds['Gender'] = ds['Gender'].apply(gender_encoding)

useful_Data = ds.drop(['State','Year','Type_code','Type','Age_group'],axis='columns')
useful_Data = useful_Data[['Cataegory','Total','Gender']]
useful_Data

x1 = useful_Data.drop(['Gender'],axis='columns')
y1 = useful_Data.Gender

x_train, x_test, y_train, y_test = train_test_split(x1,y1,test_size=0.1)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)

y_pred = gnb.predict(x_test)
from sklearn import metrics
print("Gaussian Naive Bayes model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)

y_test.value_counts()

"""Logistic regression."""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(x_train, y_train)
y_pred = logreg.predict(x_test)
accuracy = metrics.accuracy_score(y_test, y_pred) * 100
mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
r2 = metrics.r2_score(y_test, y_pred)

print(f"Linear Regression model performance:")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")
print(f"Logistic Regression model accuracy (in %): {accuracy}")

"""LinearRegression"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()

# Train the model
lin_reg.fit(x_train, y_train)

# Predict the target on the test data
y_pred = lin_reg.predict(x_test)
# accuracy = metrics.accuracy_score(y_test, y_pred) * 100
# Evaluate the model
mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
r2 = metrics.r2_score(y_test, y_pred)
print(f"Linear Regression model performance:")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")
# print(f"Logistic Regression model accuracy (in %): {accuracy}")
tolerance = 0.10

# Calculate the accuracy as the percentage of predictions within the tolerance
accuracy_within_tolerance = sum(abs((y_pred - y_test) / y_test) <= tolerance) / len(y_test) * 100

print(f"Accuracy within {tolerance*100}% tolerance: {accuracy_within_tolerance:.2f}%")

"""svm"""

from sklearn.svm import SVR

# Initialize the SVR model with RBF kernel
svr_reg = SVR(kernel='rbf')

# Train the model
svr_reg.fit(x_train, y_train)

# Predict the target on the test data
y_pred = svr_reg.predict(x_test)

# Evaluate the model
mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
r2 = metrics.r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")
# print(f"SVM model accuracy (in %): {accuracy}")
tolerance = 0.10

# Calculate the accuracy as the percentage of predictions within the tolerance
accuracy_within_tolerance = sum(abs((y_pred - y_test) / y_test) <= tolerance) / len(y_test) * 100

print(f"Accuracy within {tolerance*100}% tolerance: {accuracy_within_tolerance:.2f}%")

"""RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor

# Initialize the RandomForestRegressor
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the Random Forest Regressor
rf_reg.fit(x_train, y_train)

# Predict the target on the test data
y_pred = rf_reg.predict(x_test)

# Evaluate the model
mse = metrics.mean_squared_error(y_test, y_pred)
mae = metrics.mean_absolute_error(y_test, y_pred)
r2 = metrics.r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")
# print(f"Logistic Regression model accuracy (in %): {accuracy}")
tolerance = 0.10

# Calculate the accuracy as the percentage of predictions within the tolerance
accuracy_within_tolerance = sum(abs((y_pred - y_test) / y_test) <= tolerance) / len(y_test) * 100

print(f"Accuracy within {tolerance*100}% tolerance: {accuracy_within_tolerance:.2f}%")

"""VotingClassifier"""

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

# Initialize individual classifiers
log_reg = LogisticRegression()
svc = SVC(probability=True)  # Enable probability estimates for soft voting
decision_tree = DecisionTreeClassifier()

# Create a voting classifier
voting_clf = VotingClassifier(
    estimators=[
        ('log_reg', log_reg),
        ('svc', svc),
        ('decision_tree', decision_tree)
    ],
    voting='soft'  # Use 'hard' for hard voting
)

# Train the voting classifier
voting_clf.fit(x_train, y_train)

# Predict the target on the test data
y_pred = voting_clf.predict(x_test)

# Evaluate the model
accuracy = metrics.accuracy_score(y_test, y_pred) * 100
print(f"Voting Classifier accuracy (in %): {accuracy}")
# Additional evaluation metrics
print("Classification Report:\n", metrics.classification_report(y_test, y_pred))
print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, y_pred))

"""BaggingClassifier"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics


# Initialize the base model
base_model = DecisionTreeClassifier()

# Initialize the Bagging Classifier with the base model
bagging_clf = BaggingClassifier(
    estimator=base_model,  # Updated from base_estimator to estimator
    n_estimators=50,       # Number of models to train
    random_state=42
)

# Assuming x_train, y_train, x_test, and y_test are already defined

# Train the Bagging Classifier
bagging_clf.fit(x_train, y_train)

# Predict the target on the test data
y_pred = bagging_clf.predict(x_test)

# Evaluate the model
accuracy = metrics.accuracy_score(y_test, y_pred) * 100
print(f"Bagging Classifier accuracy (in %): {accuracy}")

# Additional evaluation metrics
print("Classification Report:\n", metrics.classification_report(y_test, y_pred))
print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, y_pred))

"""DecisionTreeClassifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
clf = DecisionTreeClassifier(random_state=42)
clf.fit(x_train, y_train)

# Step 4: Make predictions on the test set
y_pred = clf.predict(x_test)

# Step 5: Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print("Classification Report:\n", classification_report(y_test, y_pred))

"""KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier
# Step 3: Initialize and train the KNN classifier (using k=3)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print("Classification Report:\n", classification_report(y_test, y_pred))

"""xgboost"""

import xgboost as xgb
model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')  # Disable unnecessary warning
model.fit(x_train, y_train)

# Step 4: Make predictions on the test set
y_pred = model.predict(x_test)

# Step 5: Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print("Classification Report:\n", classification_report(y_test, y_pred))

new_data = pd.DataFrame([[4, 400]], columns=['Cataegory','Total'])
Gender_prediction = bagging_clf.predict(new_data)
if Gender_prediction == 0:
    Gender_prediction = "Female"
else:
    Gender_prediction = "Male"
print(f"Predicted Gender: {Gender_prediction}")

